{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (349024, 13)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>user_id</th>\n",
       "      <th>product</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>webpage_id</th>\n",
       "      <th>product_category_1</th>\n",
       "      <th>user_group_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_level</th>\n",
       "      <th>user_depth</th>\n",
       "      <th>var_1</th>\n",
       "      <th>is_click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98528.0</td>\n",
       "      <td>2017-07-04 16:42</td>\n",
       "      <td>7716.0</td>\n",
       "      <td>C</td>\n",
       "      <td>405490.0</td>\n",
       "      <td>60305.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>589714.0</td>\n",
       "      <td>2017-07-07 07:40</td>\n",
       "      <td>1035283.0</td>\n",
       "      <td>I</td>\n",
       "      <td>118601.0</td>\n",
       "      <td>28529.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>478652.0</td>\n",
       "      <td>2017-07-07 20:42</td>\n",
       "      <td>65994.0</td>\n",
       "      <td>H</td>\n",
       "      <td>359520.0</td>\n",
       "      <td>13787.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34536.0</td>\n",
       "      <td>2017-07-05 15:05</td>\n",
       "      <td>75976.0</td>\n",
       "      <td>H</td>\n",
       "      <td>405490.0</td>\n",
       "      <td>60305.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71863.0</td>\n",
       "      <td>2017-07-06 20:11</td>\n",
       "      <td>987498.0</td>\n",
       "      <td>C</td>\n",
       "      <td>405490.0</td>\n",
       "      <td>60305.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id          DateTime    user_id product  campaign_id  webpage_id  \\\n",
       "0     98528.0  2017-07-04 16:42     7716.0       C     405490.0     60305.0   \n",
       "1    589714.0  2017-07-07 07:40  1035283.0       I     118601.0     28529.0   \n",
       "2    478652.0  2017-07-07 20:42    65994.0       H     359520.0     13787.0   \n",
       "3     34536.0  2017-07-05 15:05    75976.0       H     405490.0     60305.0   \n",
       "4     71863.0  2017-07-06 20:11   987498.0       C     405490.0     60305.0   \n",
       "\n",
       "   product_category_1  user_group_id  gender  age_level  user_depth  var_1  \\\n",
       "0                 3.0            3.0    Male        3.0         3.0    1.0   \n",
       "1                 4.0           10.0  Female        4.0         3.0    1.0   \n",
       "2                 4.0            4.0    Male        4.0         3.0    0.0   \n",
       "3                 3.0            3.0    Male        3.0         3.0    0.0   \n",
       "4                 3.0            2.0    Male        2.0         3.0    0.0   \n",
       "\n",
       "   is_click  \n",
       "0       1.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd # type: ignore\n",
    "import constants as cons\n",
    "import numpy as np\n",
    "\n",
    "# Load the raw data - using the correct path\n",
    "\n",
    "# Clean the data using the existing clean_data function\n",
    "# Use the constants file path instead of hardcoded path\n",
    "df = pd.read_csv('../' + cons.DATA_PATH + cons.DEFAULT_RAW_TRAIN_FILE)\n",
    "df = df.drop(columns=cons.COLUMNS_TO_DROP)\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "\n",
    "# Add engineered features\n",
    "\n",
    "# Display basic information about the preprocessed dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding engineered time related features\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding engineered time related features\")\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "df['day_of_week'] = df['DateTime'].dt.dayofweek\n",
    "df['hour'] = df['DateTime'].dt.hour\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/val/test sets (60/20/20) while maintaining click distribution\n",
      "Train set size: 209414 (60.0%)\n",
      "Validation set size: 69805 (20.0%)\n",
      "Test set size: 69805 (20.0%)\n",
      "\n",
      "Click rates:\n",
      "Overall: 0.068\n",
      "Train: 0.068\n",
      "Validation: 0.068\n",
      "Test: 0.068\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting data into train/val/test sets (60/20/20) while maintaining click distribution\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Get click distribution for stratification\n",
    "y = df['is_click']\n",
    "\n",
    "# Split into train and temp sets (60% train, 40% temp)\n",
    "train_df_naive, temp_df = train_test_split(\n",
    "    df,\n",
    "    train_size=0.6,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into validation and test sets (50% each, so 20% of original data each)\n",
    "val_df_naive, test_df_naive = train_test_split(\n",
    "    temp_df,\n",
    "    train_size=0.5, \n",
    "    stratify=temp_df['is_click'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df_naive)} ({len(train_df_naive)/len(df):.1%})\")\n",
    "print(f\"Validation set size: {len(val_df_naive)} ({len(val_df_naive)/len(df):.1%})\")\n",
    "print(f\"Test set size: {len(test_df_naive)} ({len(test_df_naive)/len(df):.1%})\")\n",
    "\n",
    "print(\"\\nClick rates:\")\n",
    "print(f\"Overall: {df['is_click'].mean():.3f}\")\n",
    "print(f\"Train: {train_df_naive['is_click'].mean():.3f}\")\n",
    "print(f\"Validation: {val_df_naive['is_click'].mean():.3f}\") \n",
    "print(f\"Test: {test_df_naive['is_click'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating baseline F1 score with naive splitting\n",
      "One-hot encoding categorical features\n",
      "Training Random Forest model...\n",
      "\n",
      "F1 Scores:\n",
      "Test F1 (naive splitting): 0.151\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Calculating baseline F1 score with naive splitting\")\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "print(\"One-hot encoding categorical features\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Drop DateTime, user_id, and session_id columns first\n",
    "columns_to_drop = ['DateTime', 'user_id', 'session_id']\n",
    "train_df_processed = train_df_naive.drop(columns=columns_to_drop)\n",
    "val_df_processed = val_df_naive.drop(columns=columns_to_drop) \n",
    "test_df_processed = test_df_naive.drop(columns=columns_to_drop)\n",
    "\n",
    "# Separate features\n",
    "categorical_features = [col for col in cons.CATEGORICAL if col not in columns_to_drop]\n",
    "numeric_features = [col for col in train_df_processed.columns if col not in categorical_features + ['is_click']]\n",
    "\n",
    "# Fit and transform on training data\n",
    "X_train_encoded = encoder.fit_transform(train_df_processed[categorical_features])\n",
    "X_val_encoded = encoder.transform(val_df_processed[categorical_features])\n",
    "X_test_encoded = encoder.transform(test_df_processed[categorical_features])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Convert to DataFrames\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=feature_names, index=train_df_processed.index)\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=feature_names, index=val_df_processed.index)\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=feature_names, index=test_df_processed.index)\n",
    "\n",
    "# Add numeric columns\n",
    "X_train = pd.concat([X_train_encoded, train_df_processed[numeric_features]], axis=1)\n",
    "X_val = pd.concat([X_val_encoded, val_df_processed[numeric_features]], axis=1)\n",
    "X_test = pd.concat([X_test_encoded, test_df_processed[numeric_features]], axis=1)\n",
    "\n",
    "y_train = train_df_processed['is_click']\n",
    "y_val = val_df_processed['is_click']\n",
    "y_test = test_df_processed['is_click']\n",
    "\n",
    "# Train Random Forest model with reasonable parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10, \n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print F1 scores\n",
    "print(\"\\nF1 Scores:\")\n",
    "baseline_f1_naive = f1_score(y_test, y_test_pred)\n",
    "print(f\"Test F1 (naive splitting): {baseline_f1_naive:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data by user, maintaining click and session distribution\n",
      "Number of users in each set:\n",
      "Train: 76507 (60.0%)\n",
      "Validation: 25502 (20.0%)\n",
      "Test: 25503 (20.0%)\n",
      "\n",
      "Number of sessions in each set:\n",
      "Train: 209531 (60.0%)\n",
      "Validation: 69610 (19.9%)\n",
      "Test: 69883 (20.0%)\n",
      "\n",
      "Click rates in each set:\n",
      "Train: 0.068\n",
      "Validation: 0.068\n",
      "Test: 0.068\n",
      "\n",
      "Average sessions per user in each set:\n",
      "Train: 2.74\n",
      "Validation: 2.73\n",
      "Test: 2.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Splitting data by user, maintaining click and session distribution\")\n",
    "# Create user-level features for stratification\n",
    "user_features = df.groupby('user_id').agg({\n",
    "    'session_id': 'count',  # number of sessions\n",
    "    'is_click': 'sum'       # number of clicks (not rate)\n",
    "}).reset_index()\n",
    "\n",
    "# Create stratification group using actual values\n",
    "user_features['strat_group'] = user_features.apply(\n",
    "    lambda x: f\"sessions_{int(x['session_id'])}_clicks_{int(x['is_click'])}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Identify common and rare groups\n",
    "group_counts = user_features['strat_group'].value_counts()\n",
    "common_groups = group_counts[group_counts >= 6].index\n",
    "\n",
    "# Split users into common and rare groups\n",
    "common_users = user_features[user_features['strat_group'].isin(common_groups)]\n",
    "rare_users = user_features[~user_features['strat_group'].isin(common_groups)]\n",
    "\n",
    "# Split common users with stratification\n",
    "train_users_common, temp_users_common = train_test_split(\n",
    "    common_users['user_id'],\n",
    "    train_size=0.6,\n",
    "    stratify=common_users['strat_group'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_users_common, test_users_common = train_test_split(\n",
    "    temp_users_common,\n",
    "    train_size=0.5,\n",
    "    stratify=common_users.loc[common_users['user_id'].isin(temp_users_common), 'strat_group'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Randomly assign rare users to maintain approximately 60-20-20 split\n",
    "rare_users_shuffled = rare_users['user_id'].sample(frac=1, random_state=42)\n",
    "n_rare = len(rare_users_shuffled)\n",
    "n_train_rare = int(0.6 * n_rare)\n",
    "n_val_rare = int(0.2 * n_rare)\n",
    "\n",
    "train_users_rare = rare_users_shuffled[:n_train_rare]\n",
    "val_users_rare = rare_users_shuffled[n_train_rare:n_train_rare + n_val_rare]\n",
    "test_users_rare = rare_users_shuffled[n_train_rare + n_val_rare:]\n",
    "\n",
    "# Combine common and rare users\n",
    "train_users = pd.concat([train_users_common, train_users_rare])\n",
    "val_users = pd.concat([val_users_common, val_users_rare])\n",
    "test_users = pd.concat([test_users_common, test_users_rare])\n",
    "\n",
    "# Create the final dataframes\n",
    "df_train = df[df['user_id'].isin(train_users)].copy()\n",
    "df_val = df[df['user_id'].isin(val_users)].copy()\n",
    "df_test = df[df['user_id'].isin(test_users)].copy()\n",
    "\n",
    "# Print statistics to verify the split\n",
    "print(\"Number of users in each set:\")\n",
    "print(f\"Train: {len(train_users)} ({len(train_users)/len(user_features):.1%})\")\n",
    "print(f\"Validation: {len(val_users)} ({len(val_users)/len(user_features):.1%})\")\n",
    "print(f\"Test: {len(test_users)} ({len(test_users)/len(user_features):.1%})\")\n",
    "\n",
    "print(\"\\nNumber of sessions in each set:\")\n",
    "print(f\"Train: {len(df_train)} ({len(df_train)/len(df):.1%})\")\n",
    "print(f\"Validation: {len(df_val)} ({len(df_val)/len(df):.1%})\")\n",
    "print(f\"Test: {len(df_test)} ({len(df_test)/len(df):.1%})\")\n",
    "\n",
    "# Verify click distributions are similar\n",
    "print(\"\\nClick rates in each set:\")\n",
    "print(f\"Train: {df_train['is_click'].mean():.3f}\")\n",
    "print(f\"Validation: {df_val['is_click'].mean():.3f}\")\n",
    "print(f\"Test: {df_test['is_click'].mean():.3f}\")\n",
    "\n",
    "# Print distribution of sessions per user in each set\n",
    "print(\"\\nAverage sessions per user in each set:\")\n",
    "print(f\"Train: {df_train.groupby('user_id')['session_id'].count().mean():.2f}\")\n",
    "print(f\"Validation: {df_val.groupby('user_id')['session_id'].count().mean():.2f}\")\n",
    "print(f\"Test: {df_test.groupby('user_id')['session_id'].count().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding product history feature to train set...\n",
      "Adding product history feature to validation set...\n",
      "Adding product history feature to test set...\n"
     ]
    }
   ],
   "source": [
    "# Create feature for whether user has viewed product before\n",
    "def add_product_history(df):\n",
    "    # Sort by user and datetime\n",
    "    df = df.sort_values(['user_id', 'DateTime'])\n",
    "    \n",
    "    # Initialize the new feature\n",
    "    df['product_viewed_before'] = 0\n",
    "    \n",
    "    # For each user\n",
    "    for user_id in df['user_id'].unique():\n",
    "        user_sessions = df[df['user_id'] == user_id]\n",
    "        \n",
    "        # For each session of this user (already sorted chronologically)\n",
    "        for i, (_, current_session) in enumerate(user_sessions.iterrows()):\n",
    "            if i > 0:  # Skip first session\n",
    "                # Get all previous sessions for this user\n",
    "                previous_sessions = user_sessions.iloc[:i]\n",
    "                # Check if current product was viewed in any previous session\n",
    "                if current_session['product'] in previous_sessions['product'].values:\n",
    "                    df.loc[current_session.name, 'product_viewed_before'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add the feature to each dataset\n",
    "print(\"Adding product history feature to train set...\")\n",
    "df_train = add_product_history(df_train)\n",
    "\n",
    "print(\"Adding product history feature to validation set...\")\n",
    "df_val = add_product_history(df_val)\n",
    "\n",
    "print(\"Adding product history feature to test set...\")\n",
    "df_test = add_product_history(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Initialize OneHotEncoder for categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit encoder on train set categorical columns\n",
    "encoder.fit(df_train[cons.CATEGORICAL])\n",
    "\n",
    "# Transform train set\n",
    "train_cat_encoded = encoder.transform(df_train[cons.CATEGORICAL])\n",
    "train_cat_cols = encoder.get_feature_names_out(cons.CATEGORICAL)\n",
    "df_train_encoded = pd.concat([\n",
    "    df_train.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(train_cat_encoded, columns=train_cat_cols, index=df_train.index)\n",
    "], axis=1)\n",
    "\n",
    "# Transform validation set using fitted encoder\n",
    "val_cat_encoded = encoder.transform(df_val[cons.CATEGORICAL]) \n",
    "df_val_encoded = pd.concat([\n",
    "    df_val.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(val_cat_encoded, columns=train_cat_cols, index=df_val.index)\n",
    "], axis=1)\n",
    "\n",
    "# Transform test set using fitted encoder\n",
    "test_cat_encoded = encoder.transform(df_test[cons.CATEGORICAL])\n",
    "df_test_encoded = pd.concat([\n",
    "    df_test.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(test_cat_encoded, columns=train_cat_cols, index=df_test.index)\n",
    "], axis=1)\n",
    "\n",
    "df_train = df_train_encoded\n",
    "df_val = df_val_encoded\n",
    "df_test = df_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model F1 Score: 0.152\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate baseline F1 score without product history feature\n",
    "baseline_features = [col for col in df_train.columns \n",
    "                    if col not in ['user_id', 'session_id', 'DateTime', 'is_click', 'product_viewed_before']]\n",
    "\n",
    "# Initialize baseline model with reasonable parameters\n",
    "baseline_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10, \n",
    "    min_samples_split=5,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "baseline_model.fit(df_train[baseline_features], df_train['is_click'])\n",
    "\n",
    "# Generate predictions\n",
    "baseline_predictions = baseline_model.predict(df_test[baseline_features])\n",
    "\n",
    "# Calculate and print F1 score\n",
    "baseline_f1 = f1_score(df_test['is_click'], baseline_predictions)\n",
    "print(f\"Baseline Model F1 Score: {baseline_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set F1 Scores:\n",
      "With product_viewed_before:    0.1605\n",
      "Without product_viewed_before: 0.1519\n",
      "Improvement:                   5.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Features to exclude from model\n",
    "exclude_cols = ['user_id', 'session_id', 'DateTime']\n",
    "\n",
    "# Get feature columns with and without product history\n",
    "features_with_history = [col for col in df_train.columns if col not in exclude_cols + ['is_click']]\n",
    "features_without_history = [col for col in features_with_history if col != 'product_viewed_before']\n",
    "\n",
    "# Initialize models\n",
    "model_with_history = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, \n",
    "                                          class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train and evaluate model with product history\n",
    "model_with_history.fit(df_train[features_with_history], df_train['is_click'])\n",
    "y_pred_with_history = model_with_history.predict(df_test[features_with_history])\n",
    "f1_with_history = f1_score(df_test['is_click'], y_pred_with_history)\n",
    "\n",
    "\n",
    "print(\"Test Set F1 Scores:\")\n",
    "print(f\"With product_viewed_before:    {f1_with_history:.4f}\")\n",
    "print(f\"Without product_viewed_before: {baseline_f1:.4f}\")\n",
    "print(f\"Improvement:                   {((f1_with_history - baseline_f1) / baseline_f1 * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting datasets into initial and final sessions...\n",
      "\n",
      "Dataset sizes after splitting:\n",
      "Training set:\n",
      "  Initial sessions: 133024 rows\n",
      "  Final sessions:   76507 rows\n",
      "  Total users:      76507\n",
      "\n",
      "Validation set:\n",
      "  Initial sessions: 44108 rows\n",
      "  Final sessions:   25502 rows\n",
      "  Total users:      25502\n",
      "\n",
      "Test set:\n",
      "  Initial sessions: 44380 rows\n",
      "  Final sessions:   25503 rows\n",
      "  Total users:      25503\n"
     ]
    }
   ],
   "source": [
    "# Function to split dataframe into initial and final sessions for each user\n",
    "def split_initial_final_sessions(df):\n",
    "    # Sort by user_id and DateTime to ensure chronological order\n",
    "    df = df.sort_values(['user_id', 'DateTime'])\n",
    "    \n",
    "    # Get the last session for each user\n",
    "    final_sessions = df.groupby('user_id').last().reset_index()\n",
    "    \n",
    "    # Get all sessions except the last one for each user\n",
    "    initial_sessions = df.merge(\n",
    "        final_sessions[['user_id', 'session_id']], \n",
    "        on='user_id', \n",
    "        how='left', \n",
    "        suffixes=('', '_final')\n",
    "    )\n",
    "    initial_sessions = initial_sessions[\n",
    "        initial_sessions['session_id'] != initial_sessions['session_id_final']\n",
    "    ].drop('session_id_final', axis=1)\n",
    "    \n",
    "    return initial_sessions, final_sessions\n",
    "\n",
    "# Split each dataset into initial and final sessions\n",
    "print(\"Splitting datasets into initial and final sessions...\")\n",
    "\n",
    "train_initial, train_final = split_initial_final_sessions(df_train)\n",
    "val_initial, val_final = split_initial_final_sessions(df_val)\n",
    "test_initial, test_final = split_initial_final_sessions(df_test)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nDataset sizes after splitting:\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Initial sessions: {len(train_initial)} rows\")\n",
    "print(f\"  Final sessions:   {len(train_final)} rows\")\n",
    "print(f\"  Total users:      {train_final['user_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Initial sessions: {len(val_initial)} rows\")\n",
    "print(f\"  Final sessions:   {len(val_final)} rows\")\n",
    "print(f\"  Total users:      {val_final['user_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Initial sessions: {len(test_initial)} rows\")\n",
    "print(f\"  Final sessions:   {len(test_final)} rows\")\n",
    "print(f\"  Total users:      {test_final['user_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from xgboost) (2.2.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from xgboost) (1.15.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with alpha = 0.1\n",
      "F1 score: 0.1425\n",
      "\n",
      "Testing with alpha = 0.2\n",
      "F1 score: 0.1425\n",
      "\n",
      "Testing with alpha = 0.5\n",
      "F1 score: 0.1425\n",
      "\n",
      "Testing with alpha = 0.8\n",
      "F1 score: 0.1430\n",
      "\n",
      "Testing with alpha = 0.9\n",
      "F1 score: 0.1430\n",
      "\n",
      "Best F1 score: 0.1430\n",
      "F1 score with combined training: 0.1426\n",
      "F1 score with recursive forecasting: 0.1430\n",
      "Difference: -0.0005 (-0.3%)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# First, get the final session for each user\n",
    "test_final = df_test.sort_values(['user_id', 'DateTime']).groupby('user_id').last().reset_index()\n",
    "train_final = df_train.sort_values(['user_id', 'DateTime']).groupby('user_id').last().reset_index()\n",
    "\n",
    "# Get all other sessions for initial\n",
    "test_initial = df_test[~df_test['session_id'].isin(test_final['session_id'])]\n",
    "train_initial = df_train[~df_train['session_id'].isin(train_final['session_id'])]\n",
    "\n",
    "# Define columns to drop only for model input\n",
    "model_cols_to_drop = ['DateTime', 'user_id', 'session_id']\n",
    "\n",
    "# Initialize models with parameters for high class imbalance\n",
    "model_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'scale_pos_weight': 20,  # ~1/0.00068 to handle class imbalance\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "xgb_initial = XGBClassifier(**model_params)\n",
    "xgb_final = XGBClassifier(**model_params)\n",
    "\n",
    "# Keep full features for calculations\n",
    "X_train_initial_full = train_initial.drop(['is_click'], axis=1)\n",
    "# Drop user_id only for model\n",
    "X_train_initial_model = X_train_initial_full.drop(model_cols_to_drop, axis=1)\n",
    "y_train_initial = train_initial['is_click']\n",
    "\n",
    "xgb_initial.fit(X_train_initial_model, y_train_initial)\n",
    "\n",
    "# Get probabilities for train_initial\n",
    "train_initial_probs = xgb_initial.predict_proba(X_train_initial_model)[:, 1]\n",
    "\n",
    "# Calculate user-level mean probabilities using the full features\n",
    "user_probs = pd.DataFrame({\n",
    "    'user_id': X_train_initial_full['user_id'],\n",
    "    'mean_prob': train_initial_probs\n",
    "}).groupby('user_id')['mean_prob'].mean()\n",
    "\n",
    "# Overall mean probability for users with no history\n",
    "global_mean = train_initial_probs.mean()\n",
    "\n",
    "results = []\n",
    "for alpha in [0.1, 0.2, 0.5, 0.8, 0.9]:\n",
    "    print(f\"\\nTesting with alpha = {alpha}\")\n",
    "    \n",
    "    # Keep full features for calculations\n",
    "    X_train_final_full = train_final.drop(['is_click'], axis=1).copy()\n",
    "    X_train_final_full['user_mean_prob'] = (\n",
    "        alpha * X_train_final_full['user_id'].map(user_probs).fillna(global_mean) +\n",
    "        (1 - alpha) * global_mean\n",
    "    )\n",
    "    # Create model features by dropping user_id\n",
    "    X_train_final_model = X_train_final_full.drop(model_cols_to_drop, axis=1)\n",
    "    y_train_final = train_final['is_click']\n",
    "    \n",
    "    # Train final model\n",
    "    xgb_final.fit(X_train_final_model, y_train_final)\n",
    "    \n",
    "    # Keep full features for test set calculations\n",
    "    X_test_initial_full = test_initial.drop(['is_click'], axis=1)\n",
    "    X_test_initial_model = X_test_initial_full.drop(model_cols_to_drop, axis=1)\n",
    "    test_initial_probs = xgb_initial.predict_proba(X_test_initial_model)[:, 1]\n",
    "    \n",
    "    # Calculate user-level mean probabilities for test set\n",
    "    test_user_probs = pd.DataFrame({\n",
    "        'user_id': X_test_initial_full['user_id'],\n",
    "        'mean_prob': test_initial_probs\n",
    "    }).groupby('user_id')['mean_prob'].mean()\n",
    "    \n",
    "    # Add mean probabilities to test_final\n",
    "    X_test_final_full = test_final.drop(['is_click'], axis=1).copy()\n",
    "    X_test_final_full['user_mean_prob'] = (\n",
    "        alpha * X_test_final_full['user_id'].map(test_user_probs).fillna(global_mean) +\n",
    "        (1 - alpha) * global_mean\n",
    "    )\n",
    "    X_test_final_model = X_test_final_full.drop(model_cols_to_drop, axis=1)\n",
    "    \n",
    "    # Get predictions\n",
    "    initial_preds = xgb_initial.predict(X_test_initial_model)\n",
    "    final_preds = xgb_final.predict(X_test_final_model)\n",
    "    \n",
    "    # Create a dictionary mapping session_id to prediction\n",
    "    initial_pred_dict = dict(zip(test_initial['session_id'], initial_preds))\n",
    "    final_pred_dict = dict(zip(test_final['session_id'], final_preds))\n",
    "    \n",
    "    # Combine all predictions using the original df_test order\n",
    "    all_preds = df_test['session_id'].map({**initial_pred_dict, **final_pred_dict})\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    overall_f1 = f1_score(df_test['is_click'], all_preds)\n",
    "    results.append({'alpha': alpha, 'overall_f1': overall_f1})\n",
    "    \n",
    "    print(f\"F1 score: {overall_f1:.4f}\")\n",
    "\n",
    "best_f1 = max([r['overall_f1'] for r in results])\n",
    "print(f\"\\nBest F1 score: {best_f1:.4f}\")\n",
    "# Combine train sets\n",
    "X_train_combined = pd.concat([\n",
    "    train_initial.drop(['is_click'], axis=1).drop(model_cols_to_drop, axis=1),\n",
    "    train_final.drop(['is_click'], axis=1).drop(model_cols_to_drop, axis=1)\n",
    "])\n",
    "y_train_combined = pd.concat([train_initial['is_click'], train_final['is_click']])\n",
    "\n",
    "# Train XGBoost on combined data\n",
    "xgb_combined = XGBClassifier(**model_params)\n",
    "xgb_combined.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# Combine test sets\n",
    "X_test_combined = pd.concat([\n",
    "    test_initial.drop(['is_click'], axis=1).drop(model_cols_to_drop, axis=1),\n",
    "    test_final.drop(['is_click'], axis=1).drop(model_cols_to_drop, axis=1)\n",
    "])\n",
    "y_test_combined = pd.concat([test_initial['is_click'], test_final['is_click']])\n",
    "\n",
    "# Get predictions on combined test set\n",
    "combined_preds = xgb_combined.predict(X_test_combined)\n",
    "combined_f1 = f1_score(y_test_combined, combined_preds)\n",
    "\n",
    "print(f\"F1 score with combined training: {combined_f1:.4f}\")\n",
    "print(f\"F1 score with recursive forecasting: {best_f1:.4f}\")\n",
    "print(f\"Difference: {(combined_f1 - best_f1):.4f} ({(combined_f1/best_f1 - 1)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
