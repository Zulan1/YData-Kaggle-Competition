{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (349024, 13)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>user_id</th>\n",
       "      <th>product</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>webpage_id</th>\n",
       "      <th>product_category_1</th>\n",
       "      <th>user_group_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_level</th>\n",
       "      <th>user_depth</th>\n",
       "      <th>var_1</th>\n",
       "      <th>is_click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98528.0</td>\n",
       "      <td>2017-07-04 16:42</td>\n",
       "      <td>7716.0</td>\n",
       "      <td>C</td>\n",
       "      <td>405490.0</td>\n",
       "      <td>60305.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>589714.0</td>\n",
       "      <td>2017-07-07 07:40</td>\n",
       "      <td>1035283.0</td>\n",
       "      <td>I</td>\n",
       "      <td>118601.0</td>\n",
       "      <td>28529.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>478652.0</td>\n",
       "      <td>2017-07-07 20:42</td>\n",
       "      <td>65994.0</td>\n",
       "      <td>H</td>\n",
       "      <td>359520.0</td>\n",
       "      <td>13787.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34536.0</td>\n",
       "      <td>2017-07-05 15:05</td>\n",
       "      <td>75976.0</td>\n",
       "      <td>H</td>\n",
       "      <td>405490.0</td>\n",
       "      <td>60305.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71863.0</td>\n",
       "      <td>2017-07-06 20:11</td>\n",
       "      <td>987498.0</td>\n",
       "      <td>C</td>\n",
       "      <td>405490.0</td>\n",
       "      <td>60305.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id          DateTime    user_id product  campaign_id  webpage_id  \\\n",
       "0     98528.0  2017-07-04 16:42     7716.0       C     405490.0     60305.0   \n",
       "1    589714.0  2017-07-07 07:40  1035283.0       I     118601.0     28529.0   \n",
       "2    478652.0  2017-07-07 20:42    65994.0       H     359520.0     13787.0   \n",
       "3     34536.0  2017-07-05 15:05    75976.0       H     405490.0     60305.0   \n",
       "4     71863.0  2017-07-06 20:11   987498.0       C     405490.0     60305.0   \n",
       "\n",
       "   product_category_1  user_group_id  gender  age_level  user_depth  var_1  \\\n",
       "0                 3.0            3.0    Male        3.0         3.0    1.0   \n",
       "1                 4.0           10.0  Female        4.0         3.0    1.0   \n",
       "2                 4.0            4.0    Male        4.0         3.0    0.0   \n",
       "3                 3.0            3.0    Male        3.0         3.0    0.0   \n",
       "4                 3.0            2.0    Male        2.0         3.0    0.0   \n",
       "\n",
       "   is_click  \n",
       "0       1.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd # type: ignore\n",
    "import constants as cons\n",
    "import numpy as np\n",
    "\n",
    "# Load the raw data - using the correct path\n",
    "\n",
    "# Clean the data using the existing clean_data function\n",
    "# Use the constants file path instead of hardcoded path\n",
    "df = pd.read_csv('../' + cons.DATA_PATH + cons.DEFAULT_RAW_TRAIN_FILE)\n",
    "df = df.drop(columns=cons.COLUMNS_TO_DROP)\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "\n",
    "# Add engineered features\n",
    "\n",
    "# Display basic information about the preprocessed dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding engineered time related features\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding engineered time related features\")\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "df['day_of_week'] = df['DateTime'].dt.dayofweek\n",
    "df['hour'] = df['DateTime'].dt.hour\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing users with more than 10 sessions -- likely bots\n"
     ]
    }
   ],
   "source": [
    "print(f\"Removing users with more than 10 sessions -- likely bots\")\n",
    "# Count sessions per user\n",
    "session_counts = df.groupby('user_id')['session_id'].count()\n",
    "\n",
    "# Get users with 10 or fewer sessions\n",
    "valid_users = session_counts[session_counts <= 10].index\n",
    "\n",
    "# Filter dataframe to only include those users\n",
    "df = df[df['user_id'].isin(valid_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data by user, maintaining click and session distribution\n",
      "Number of users in each set:\n",
      "Train: 73924 (60.0%)\n",
      "Validation: 24642 (20.0%)\n",
      "Test: 24642 (20.0%)\n",
      "\n",
      "Number of sessions in each set:\n",
      "Train: 156266 (60.0%)\n",
      "Validation: 52277 (20.1%)\n",
      "Test: 51932 (19.9%)\n",
      "\n",
      "Click rates in each set:\n",
      "Train: 0.076\n",
      "Validation: 0.076\n",
      "Test: 0.076\n",
      "\n",
      "Average sessions per user in each set:\n",
      "Train: 2.11\n",
      "Validation: 2.12\n",
      "Test: 2.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Splitting data by user, maintaining click and session distribution\")\n",
    "# Create user-level features for stratification\n",
    "user_features = df.groupby('user_id').agg({\n",
    "    'session_id': 'count',  # number of sessions\n",
    "    'is_click': 'sum'       # number of clicks (not rate)\n",
    "}).reset_index()\n",
    "\n",
    "# Create stratification group using actual values\n",
    "user_features['strat_group'] = user_features.apply(\n",
    "    lambda x: f\"sessions_{int(x['session_id'])}_clicks_{int(x['is_click'])}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Identify common and rare groups\n",
    "group_counts = user_features['strat_group'].value_counts()\n",
    "common_groups = group_counts[group_counts >= 6].index\n",
    "\n",
    "# Split users into common and rare groups\n",
    "common_users = user_features[user_features['strat_group'].isin(common_groups)]\n",
    "rare_users = user_features[~user_features['strat_group'].isin(common_groups)]\n",
    "\n",
    "# Split common users with stratification\n",
    "train_users_common, temp_users_common = train_test_split(\n",
    "    common_users['user_id'],\n",
    "    train_size=0.6,\n",
    "    stratify=common_users['strat_group'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_users_common, test_users_common = train_test_split(\n",
    "    temp_users_common,\n",
    "    train_size=0.5,\n",
    "    stratify=common_users.loc[common_users['user_id'].isin(temp_users_common), 'strat_group'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Randomly assign rare users to maintain approximately 60-20-20 split\n",
    "rare_users_shuffled = rare_users['user_id'].sample(frac=1, random_state=42)\n",
    "n_rare = len(rare_users_shuffled)\n",
    "n_train_rare = int(0.6 * n_rare)\n",
    "n_val_rare = int(0.2 * n_rare)\n",
    "\n",
    "train_users_rare = rare_users_shuffled[:n_train_rare]\n",
    "val_users_rare = rare_users_shuffled[n_train_rare:n_train_rare + n_val_rare]\n",
    "test_users_rare = rare_users_shuffled[n_train_rare + n_val_rare:]\n",
    "\n",
    "# Combine common and rare users\n",
    "train_users = pd.concat([train_users_common, train_users_rare])\n",
    "val_users = pd.concat([val_users_common, val_users_rare])\n",
    "test_users = pd.concat([test_users_common, test_users_rare])\n",
    "\n",
    "# Create the final dataframes\n",
    "df_train = df[df['user_id'].isin(train_users)].copy()\n",
    "df_val = df[df['user_id'].isin(val_users)].copy()\n",
    "df_test = df[df['user_id'].isin(test_users)].copy()\n",
    "\n",
    "# Print statistics to verify the split\n",
    "print(\"Number of users in each set:\")\n",
    "print(f\"Train: {len(train_users)} ({len(train_users)/len(user_features):.1%})\")\n",
    "print(f\"Validation: {len(val_users)} ({len(val_users)/len(user_features):.1%})\")\n",
    "print(f\"Test: {len(test_users)} ({len(test_users)/len(user_features):.1%})\")\n",
    "\n",
    "print(\"\\nNumber of sessions in each set:\")\n",
    "print(f\"Train: {len(df_train)} ({len(df_train)/len(df):.1%})\")\n",
    "print(f\"Validation: {len(df_val)} ({len(df_val)/len(df):.1%})\")\n",
    "print(f\"Test: {len(df_test)} ({len(df_test)/len(df):.1%})\")\n",
    "\n",
    "# Verify click distributions are similar\n",
    "print(\"\\nClick rates in each set:\")\n",
    "print(f\"Train: {df_train['is_click'].mean():.3f}\")\n",
    "print(f\"Validation: {df_val['is_click'].mean():.3f}\")\n",
    "print(f\"Test: {df_test['is_click'].mean():.3f}\")\n",
    "\n",
    "# Print distribution of sessions per user in each set\n",
    "print(\"\\nAverage sessions per user in each set:\")\n",
    "print(f\"Train: {df_train.groupby('user_id')['session_id'].count().mean():.2f}\")\n",
    "print(f\"Validation: {df_val.groupby('user_id')['session_id'].count().mean():.2f}\")\n",
    "print(f\"Test: {df_test.groupby('user_id')['session_id'].count().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding product history feature to train set...\n",
      "Adding product history feature to validation set...\n",
      "Adding product history feature to test set...\n",
      "\n",
      "Product viewed before rates in each set:\n",
      "Train: 0.305\n",
      "Validation: 0.307\n",
      "Test: 0.302\n"
     ]
    }
   ],
   "source": [
    "# Create feature for whether user has viewed product before\n",
    "def add_product_history(df):\n",
    "    # Sort by user and datetime\n",
    "    df = df.sort_values(['user_id', 'DateTime'])\n",
    "    \n",
    "    # Initialize the new feature\n",
    "    df['product_viewed_before'] = 0\n",
    "    \n",
    "    # For each user\n",
    "    for user_id in df['user_id'].unique():\n",
    "        user_sessions = df[df['user_id'] == user_id]\n",
    "        \n",
    "        # For each session of this user (already sorted chronologically)\n",
    "        for i, (_, current_session) in enumerate(user_sessions.iterrows()):\n",
    "            if i > 0:  # Skip first session\n",
    "                # Get all previous sessions for this user\n",
    "                previous_sessions = user_sessions.iloc[:i]\n",
    "                # Check if current product was viewed in any previous session\n",
    "                if current_session['product'] in previous_sessions['product'].values:\n",
    "                    df.loc[current_session.name, 'product_viewed_before'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add the feature to each dataset\n",
    "print(\"Adding product history feature to train set...\")\n",
    "df_train = add_product_history(df_train)\n",
    "\n",
    "print(\"Adding product history feature to validation set...\")\n",
    "df_val = add_product_history(df_val)\n",
    "\n",
    "print(\"Adding product history feature to test set...\")\n",
    "df_test = add_product_history(df_test)\n",
    "\n",
    "# Verify the feature was added correctly\n",
    "print(\"\\nProduct viewed before rates in each set:\")\n",
    "print(f\"Train: {df_train['product_viewed_before'].mean():.3f}\")\n",
    "print(f\"Validation: {df_val['product_viewed_before'].mean():.3f}\")\n",
    "print(f\"Test: {df_test['product_viewed_before'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Initialize OneHotEncoder for categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit encoder on train set categorical columns\n",
    "encoder.fit(df_train[cons.CATEGORICAL])\n",
    "\n",
    "# Transform train set\n",
    "train_cat_encoded = encoder.transform(df_train[cons.CATEGORICAL])\n",
    "train_cat_cols = encoder.get_feature_names_out(cons.CATEGORICAL)\n",
    "df_train_encoded = pd.concat([\n",
    "    df_train.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(train_cat_encoded, columns=train_cat_cols, index=df_train.index)\n",
    "], axis=1)\n",
    "\n",
    "# Transform validation set using fitted encoder\n",
    "val_cat_encoded = encoder.transform(df_val[cons.CATEGORICAL]) \n",
    "df_val_encoded = pd.concat([\n",
    "    df_val.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(val_cat_encoded, columns=train_cat_cols, index=df_val.index)\n",
    "], axis=1)\n",
    "\n",
    "# Transform test set using fitted encoder\n",
    "test_cat_encoded = encoder.transform(df_test[cons.CATEGORICAL])\n",
    "df_test_encoded = pd.concat([\n",
    "    df_test.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(test_cat_encoded, columns=train_cat_cols, index=df_test.index)\n",
    "], axis=1)\n",
    "\n",
    "df_train = df_train_encoded\n",
    "df_val = df_val_encoded\n",
    "df_test = df_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set F1 Scores:\n",
      "With product_viewed_before:    0.1604\n",
      "Without product_viewed_before: 0.1558\n",
      "Ratio:                         1.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Features to exclude from model\n",
    "exclude_cols = ['user_id', 'session_id', 'DateTime']\n",
    "\n",
    "# Get feature columns with and without product history\n",
    "features_with_history = [col for col in df_train.columns if col not in exclude_cols + ['is_click']]\n",
    "features_without_history = [col for col in features_with_history if col != 'product_viewed_before']\n",
    "\n",
    "# Initialize models\n",
    "model_with_history = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, \n",
    "                                          class_weight='balanced', random_state=42)\n",
    "model_without_history = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5,\n",
    "                                             class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train and evaluate model with product history\n",
    "model_with_history.fit(df_train[features_with_history], df_train['is_click'])\n",
    "y_pred_with_history = model_with_history.predict(df_test[features_with_history])\n",
    "f1_with_history = f1_score(df_test['is_click'], y_pred_with_history)\n",
    "\n",
    "# Train and evaluate model without product history\n",
    "model_without_history.fit(df_train[features_without_history], df_train['is_click'])\n",
    "y_pred_without_history = model_without_history.predict(df_test[features_without_history])\n",
    "f1_without_history = f1_score(df_test['is_click'], y_pred_without_history)\n",
    "\n",
    "print(\"Test Set F1 Scores:\")\n",
    "print(f\"With product_viewed_before:    {f1_with_history:.4f}\")\n",
    "print(f\"Without product_viewed_before: {f1_without_history:.4f}\")\n",
    "print(f\"Improvement:                   {((f1_with_history - f1_without_history) / f1_without_history * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions in first/returning splits:\n",
      "Train first: 73924 (47.3%)\n",
      "Train returning: 82342 (52.7%)\n",
      "\n",
      "Val first: 24642 (47.1%)\n",
      "Val returning: 27635 (52.9%)\n",
      "\n",
      "Test first: 24642 (47.5%)\n",
      "Test returning: 27290 (52.5%)\n"
     ]
    }
   ],
   "source": [
    "# Sort sessions chronologically for each user\n",
    "df_train = df_train.sort_values(['user_id', 'DateTime'])\n",
    "df_val = df_val.sort_values(['user_id', 'DateTime'])\n",
    "df_test = df_test.sort_values(['user_id', 'DateTime'])\n",
    "\n",
    "\n",
    "\n",
    "# Split train set into first and returning sessions\n",
    "train_first = df_train.groupby('user_id').first().reset_index()\n",
    "train_returning = df_train.merge(train_first[['user_id', 'session_id']], \n",
    "                                on='user_id', \n",
    "                                suffixes=('', '_first'))\n",
    "train_returning = train_returning[train_returning['session_id'] != train_returning['session_id_first']]\n",
    "train_returning = train_returning.drop('session_id_first', axis=1)\n",
    "\n",
    "# Split validation set into first and returning sessions\n",
    "val_first = df_val.groupby('user_id').first().reset_index()\n",
    "val_returning = df_val.merge(val_first[['user_id', 'session_id']], \n",
    "                            on='user_id',\n",
    "                            suffixes=('', '_first'))\n",
    "val_returning = val_returning[val_returning['session_id'] != val_returning['session_id_first']]\n",
    "val_returning = val_returning.drop('session_id_first', axis=1)\n",
    "\n",
    "# Split test set into first and returning sessions\n",
    "test_first = df_test.groupby('user_id').first().reset_index()\n",
    "test_returning = df_test.merge(test_first[['user_id', 'session_id']], \n",
    "                              on='user_id',\n",
    "                              suffixes=('', '_first'))\n",
    "test_returning = test_returning[test_returning['session_id'] != test_returning['session_id_first']]\n",
    "test_returning = test_returning.drop('session_id_first', axis=1)\n",
    "\n",
    "# Print statistics about the splits\n",
    "print(\"Number of sessions in first/returning splits:\")\n",
    "print(f\"Train first: {len(train_first)} ({len(train_first)/len(df_train):.1%})\")\n",
    "print(f\"Train returning: {len(train_returning)} ({len(train_returning)/len(df_train):.1%})\")\n",
    "print(f\"\\nVal first: {len(val_first)} ({len(val_first)/len(df_val):.1%})\")\n",
    "print(f\"Val returning: {len(val_returning)} ({len(val_returning)/len(df_val):.1%})\")\n",
    "print(f\"\\nTest first: {len(test_first)} ({len(test_first)/len(df_test):.1%})\")\n",
    "print(f\"Test returning: {len(test_returning)} ({len(test_returning)/len(df_test):.1%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test first sessions: 0.183\n",
      "Mean CTR on returning sessions: 0.066\n",
      "F1 score on test returning sessions: 0.134\n",
      "Naive F1 score (first sessions model on returning): 0.127\n",
      "Overall F1 score using specialized models: 0.164\n",
      "Overall Naive F1 score using only first sessions model: 0.157\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "# Train logistic regression model on first sessions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Prepare features for first sessions model\n",
    "cols_to_drop = ['DateTime', 'user_id', 'session_id']\n",
    "X_train_first = train_first.drop(cols_to_drop + ['is_click'], axis=1)\n",
    "y_train_first = train_first['is_click']\n",
    "\n",
    "X_test_first = test_first.drop(cols_to_drop + ['is_click'], axis=1)\n",
    "y_test_first = test_first['is_click']\n",
    "\n",
    "# Train model\n",
    "first_sessions_model = LogisticRegression(C=0.01, class_weight='balanced', random_state=42)\n",
    "first_sessions_model.fit(X_train_first, y_train_first)\n",
    "\n",
    "# Get predictions and F1 score on test set\n",
    "test_first_preds = first_sessions_model.predict(X_test_first)\n",
    "print(f\"F1 score on test first sessions: {f1_score(y_test_first, test_first_preds):.3f}\")\n",
    "\n",
    "# Calculate mean CTR on returning sessions\n",
    "mean_returning_ctr = train_returning['is_click'].mean()\n",
    "print(f\"Mean CTR on returning sessions: {mean_returning_ctr:.3f}\")\n",
    "\n",
    "# Get first session features for each user in train_returning\n",
    "user_first_sessions = train_first.drop(['DateTime', 'session_id', 'is_click', 'user_id'], axis=1)\n",
    "\n",
    "# Get model predictions for first sessions\n",
    "first_session_preds = first_sessions_model.predict_proba(user_first_sessions)[:, 1]\n",
    "first_session_preds = pd.Series(first_session_preds, index=train_first['user_id'])\n",
    "\n",
    "# Map predictions to returning sessions and calculate weighted feature\n",
    "train_returning['first_session_weighted_pred'] = train_returning['user_id'].map(first_session_preds)\n",
    "train_returning['first_session_weighted_pred'] = (1 - alpha) * train_returning['first_session_weighted_pred'] + alpha * mean_returning_ctr\n",
    "\n",
    "# Prepare features for returning sessions model\n",
    "X_train_returning = train_returning.drop(cols_to_drop + ['is_click'], axis=1)\n",
    "y_train_returning = train_returning['is_click']\n",
    "\n",
    "# Train returning sessions model\n",
    "returning_sessions_model = LogisticRegression(C=0.01, class_weight='balanced', random_state=42)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "returning_sessions_model = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, \n",
    "                                                class_weight='balanced', random_state=42)\n",
    "returning_sessions_model.fit(X_train_returning, y_train_returning)\n",
    "\n",
    "# Get first session features for test returning users\n",
    "test_user_first_sessions = test_first.drop(['DateTime', 'session_id', 'is_click', 'user_id'], axis=1)\n",
    "\n",
    "# Get model predictions for test users' first sessions\n",
    "test_first_session_preds = first_sessions_model.predict_proba(test_user_first_sessions)[:, 1]\n",
    "test_first_session_preds = pd.Series(test_first_session_preds, index=test_first['user_id'])\n",
    "\n",
    "# Map predictions to test returning sessions and calculate weighted feature\n",
    "test_returning['first_session_weighted_pred'] = test_returning['user_id'].map(test_first_session_preds)\n",
    "test_returning['first_session_weighted_pred'] = (1 - alpha) * test_returning['first_session_weighted_pred'] + alpha * mean_returning_ctr\n",
    "\n",
    "# Prepare test features and get predictions\n",
    "X_test_returning = test_returning.drop(cols_to_drop + ['is_click'], axis=1)\n",
    "y_test_returning = test_returning['is_click']\n",
    "\n",
    "test_returning_preds = returning_sessions_model.predict(X_test_returning)\n",
    "print(f\"F1 score on test returning sessions: {f1_score(y_test_returning, test_returning_preds):.3f}\")\n",
    "# For comparison, get predictions from first sessions model on returning sessions\n",
    "X_test_returning_first = test_returning.drop(cols_to_drop + ['is_click', 'first_session_weighted_pred'], axis=1)\n",
    "naive_returning_preds = first_sessions_model.predict(X_test_returning_first)\n",
    "print(f\"Naive F1 score (first sessions model on returning): {f1_score(y_test_returning, naive_returning_preds):.3f}\")\n",
    "# Combine predictions from both models for overall score\n",
    "combined_preds = []\n",
    "combined_true = []\n",
    "\n",
    "# Add first session predictions\n",
    "combined_preds.extend(test_first_preds)\n",
    "combined_true.extend(y_test_first)\n",
    "\n",
    "# Add returning session predictions\n",
    "combined_preds.extend(test_returning_preds) \n",
    "combined_true.extend(y_test_returning)\n",
    "\n",
    "print(f\"Overall F1 score using specialized models: {f1_score(combined_true, combined_preds):.3f}\")\n",
    "\n",
    "# For naive approach - use first sessions model for all predictions\n",
    "naive_combined_preds = []\n",
    "naive_combined_true = []\n",
    "\n",
    "# Add first session predictions (same as above)\n",
    "naive_combined_preds.extend(test_first_preds)\n",
    "naive_combined_true.extend(y_test_first)\n",
    "\n",
    "# Add naive predictions on returning sessions\n",
    "naive_combined_preds.extend(naive_returning_preds)\n",
    "naive_combined_true.extend(y_test_returning)\n",
    "\n",
    "print(f\"Overall Naive F1 score using only first sessions model: {f1_score(naive_combined_true, naive_combined_preds):.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  session_id            DateTime  age_level  user_depth  is_click  \\\n",
      "0     64.0    313949.0 2017-07-07 17:55:00        2.0         2.0       1.0   \n",
      "1     76.0    241652.0 2017-07-06 20:11:00        2.0         3.0       0.0   \n",
      "2     97.0    419274.0 2017-07-03 07:53:00        2.0         3.0       0.0   \n",
      "3    150.0     63075.0 2017-07-05 19:08:00        3.0         3.0       0.0   \n",
      "4    156.0    582378.0 2017-07-07 03:43:00        2.0         3.0       0.0   \n",
      "\n",
      "   hour  hour_cos  hour_sin  product_viewed_before  ...  gender_Male  \\\n",
      "0    17 -0.258819 -0.965926                      0  ...          0.0   \n",
      "1    20  0.500000 -0.866025                      1  ...          1.0   \n",
      "2     7 -0.258819  0.965926                      0  ...          1.0   \n",
      "3    19  0.258819 -0.965926                      1  ...          1.0   \n",
      "4     3  0.707107  0.707107                      0  ...          1.0   \n",
      "\n",
      "   var_1_0.0  var_1_1.0  day_of_week_0  day_of_week_1  day_of_week_2  \\\n",
      "0        0.0        1.0            0.0            0.0            0.0   \n",
      "1        1.0        0.0            0.0            0.0            0.0   \n",
      "2        1.0        0.0            1.0            0.0            0.0   \n",
      "3        1.0        0.0            0.0            0.0            1.0   \n",
      "4        0.0        1.0            0.0            0.0            0.0   \n",
      "\n",
      "   day_of_week_3  day_of_week_4  day_of_week_6  weighted_pred  \n",
      "0            0.0            1.0            0.0       0.013131  \n",
      "1            1.0            0.0            0.0       0.813131  \n",
      "2            0.0            0.0            0.0       0.813131  \n",
      "3            0.0            0.0            0.0       0.013131  \n",
      "4            0.0            1.0            0.0       0.813131  \n",
      "\n",
      "[5 rows x 68 columns]\n",
      "F1 score on test returning (last) sessions: 0.166\n",
      "Naive F1 score (first sessions model on last sessions): 0.165\n"
     ]
    }
   ],
   "source": [
    "# First, get the last session for each user - this will be our \"returning\" set\n",
    "train_returning_last = df_train.sort_values(['user_id', 'DateTime']).groupby('user_id').last().reset_index()\n",
    "val_returning_last = df_val.sort_values(['user_id', 'DateTime']).groupby('user_id').last().reset_index()\n",
    "test_returning_last = df_test.sort_values(['user_id', 'DateTime']).groupby('user_id').last().reset_index()\n",
    "\n",
    "# Now get all other sessions for the \"first\" set\n",
    "train_first = pd.concat([\n",
    "    df_train[~df_train['session_id'].isin(train_returning_last['session_id'])],\n",
    "    train_first  # Include the original first sessions\n",
    "]).drop_duplicates()\n",
    "\n",
    "val_first = pd.concat([\n",
    "    df_val[~df_val['session_id'].isin(val_returning_last['session_id'])],\n",
    "    val_first  # Include the original first sessions\n",
    "]).drop_duplicates()\n",
    "\n",
    "test_first = pd.concat([\n",
    "    df_test[~df_test['session_id'].isin(test_returning_last['session_id'])],\n",
    "    test_first  # Include the original first sessions\n",
    "]).drop_duplicates()\n",
    "\n",
    "# Train first model (RandomForest) on all sessions except last\n",
    "cols_to_drop = ['DateTime', 'user_id', 'session_id']\n",
    "X_train_first = train_first.drop(cols_to_drop + ['is_click'], axis=1)\n",
    "y_train_first = train_first['is_click']\n",
    "\n",
    "first_sessions_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=10, \n",
    "    min_samples_split=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "first_sessions_model.fit(X_train_first, y_train_first)\n",
    "alpha = 0.8\n",
    "# Get predictions for all training first sessions\n",
    "train_first_preds = first_sessions_model.predict(X_train_first)\n",
    "\n",
    "# Calculate if user had ANY predicted clicks in first sessions\n",
    "user_has_predicted_click = pd.DataFrame({\n",
    "    'user_id': train_first['user_id'],\n",
    "    'pred': train_first_preds\n",
    "}).groupby('user_id')['pred'].max()  # max will be 1 if ANY session predicted click, 0 otherwise\n",
    "\n",
    "# Add binary prediction feature to train_returning_last\n",
    "train_returning_last['weighted_pred'] = train_returning_last['user_id'].map(user_has_predicted_click)\n",
    "train_returning_last['weighted_pred'] = train_returning_last['weighted_pred'] * alpha + (1 - alpha) * mean_returning_ctr\n",
    "\n",
    "# Train returning model on last sessions with the new feature\n",
    "X_train_returning = train_returning_last.drop(cols_to_drop + ['is_click'], axis=1)\n",
    "y_train_returning = train_returning_last['is_click']\n",
    "\n",
    "returning_sessions_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "returning_sessions_model.fit(X_train_returning, y_train_returning)\n",
    "\n",
    "# Get predictions for all test first sessions\n",
    "X_test_first = test_first.drop(cols_to_drop + ['is_click'], axis=1)\n",
    "test_first_preds = first_sessions_model.predict(X_test_first)\n",
    "\n",
    "# Calculate if user had ANY predicted clicks in test first sessions\n",
    "test_user_has_predicted_click = pd.DataFrame({\n",
    "    'user_id': test_first['user_id'],\n",
    "    'pred': test_first_preds\n",
    "}).groupby('user_id')['pred'].max()\n",
    "\n",
    "# Add binary prediction feature to test_returning_last\n",
    "test_returning_last['weighted_pred'] = test_returning_last['user_id'].map(test_user_has_predicted_click)\n",
    "test_returning_last['weighted_pred'] = test_returning_last['weighted_pred'] * alpha + (1 - alpha) * mean_returning_ctr\n",
    "\n",
    "print(test_returning_last.head())\n",
    "# Get predictions and evaluate\n",
    "X_test_returning = test_returning_last.drop(cols_to_drop + ['is_click'], axis=1)\n",
    "y_test_returning = test_returning_last['is_click']\n",
    "\n",
    "test_returning_preds = returning_sessions_model.predict(X_test_returning)\n",
    "print(f\"F1 score on test returning (last) sessions: {f1_score(y_test_returning, test_returning_preds):.3f}\")\n",
    "\n",
    "# For comparison, get predictions from first sessions model\n",
    "X_test_returning_first = test_returning_last.drop(cols_to_drop + ['is_click', 'weighted_pred'], axis=1)\n",
    "naive_returning_preds = first_sessions_model.predict(X_test_returning_first)\n",
    "print(f\"Naive F1 score (first sessions model on last sessions): {f1_score(y_test_returning, naive_returning_preds):.3f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
