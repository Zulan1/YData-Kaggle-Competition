{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd # type: ignore\n",
    "sys.path.append('..')\n",
    "import constants as cons\n",
    "import numpy as np\n",
    "\n",
    "from preprocess import split_by_user\n",
    "from preprocess import clean_data\n",
    "\n",
    "df = pd.read_csv('../' + cons.DATA_PATH + cons.DEFAULT_RAW_TRAIN_FILE)\n",
    "df = df.drop(columns=cons.COLUMNS_TO_DROP)\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding engineered time related features\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding engineered time related features\")\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "df['day_of_week'] = df['DateTime'].dt.dayofweek\n",
    "df['hour'] = df['DateTime'].dt.hour\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/val/test sets (60/20/20) while maintaining click distribution\n",
      "Train set size: 209414 (60.0%)\n",
      "Validation set size: 69805 (20.0%)\n",
      "Test set size: 69805 (20.0%)\n",
      "\n",
      "Click rates:\n",
      "Overall: 0.068\n",
      "Train: 0.068\n",
      "Validation: 0.068\n",
      "Test: 0.068\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting data into train/val/test sets (60/20/20) while maintaining click distribution\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Get click distribution for stratification\n",
    "y = df['is_click']\n",
    "\n",
    "# Split into train and temp sets (60% train, 40% temp)\n",
    "train_df_naive, temp_df = train_test_split(\n",
    "    df,\n",
    "    train_size=0.6,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into validation and test sets (50% each, so 20% of original data each)\n",
    "val_df_naive, test_df_naive = train_test_split(\n",
    "    temp_df,\n",
    "    train_size=0.5, \n",
    "    stratify=temp_df['is_click'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df_naive)} ({len(train_df_naive)/len(df):.1%})\")\n",
    "print(f\"Validation set size: {len(val_df_naive)} ({len(val_df_naive)/len(df):.1%})\")\n",
    "print(f\"Test set size: {len(test_df_naive)} ({len(test_df_naive)/len(df):.1%})\")\n",
    "\n",
    "print(\"\\nClick rates:\")\n",
    "print(f\"Overall: {df['is_click'].mean():.3f}\")\n",
    "print(f\"Train: {train_df_naive['is_click'].mean():.3f}\")\n",
    "print(f\"Validation: {val_df_naive['is_click'].mean():.3f}\") \n",
    "print(f\"Test: {test_df_naive['is_click'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating baseline F1 score with naive splitting\n",
      "One-hot encoding categorical features\n",
      "Training Random Forest model...\n",
      "\n",
      "F1 Scores:\n",
      "Test F1 (naive splitting): 0.151\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Calculating baseline F1 score with naive splitting\")\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "print(\"One-hot encoding categorical features\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Drop DateTime, user_id, and session_id columns first\n",
    "columns_to_drop = ['DateTime', 'user_id', 'session_id']\n",
    "train_df_processed = train_df_naive.drop(columns=columns_to_drop)\n",
    "val_df_processed = val_df_naive.drop(columns=columns_to_drop) \n",
    "test_df_processed = test_df_naive.drop(columns=columns_to_drop)\n",
    "\n",
    "# Separate features\n",
    "categorical_features = [col for col in cons.CATEGORICAL if col not in columns_to_drop]\n",
    "numeric_features = [col for col in train_df_processed.columns if col not in categorical_features + ['is_click']]\n",
    "\n",
    "# Fit and transform on training data\n",
    "X_train_encoded = encoder.fit_transform(train_df_processed[categorical_features])\n",
    "X_val_encoded = encoder.transform(val_df_processed[categorical_features])\n",
    "X_test_encoded = encoder.transform(test_df_processed[categorical_features])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Convert to DataFrames\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=feature_names, index=train_df_processed.index)\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=feature_names, index=val_df_processed.index)\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=feature_names, index=test_df_processed.index)\n",
    "\n",
    "# Add numeric columns\n",
    "X_train = pd.concat([X_train_encoded, train_df_processed[numeric_features]], axis=1)\n",
    "X_val = pd.concat([X_val_encoded, val_df_processed[numeric_features]], axis=1)\n",
    "X_test = pd.concat([X_test_encoded, test_df_processed[numeric_features]], axis=1)\n",
    "\n",
    "y_train = train_df_processed['is_click']\n",
    "y_val = val_df_processed['is_click']\n",
    "y_test = test_df_processed['is_click']\n",
    "\n",
    "# Train Random Forest model with reasonable parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10, \n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print F1 scores\n",
    "print(\"\\nF1 Scores:\")\n",
    "baseline_f1_naive = f1_score(y_test, y_test_pred)\n",
    "print(f\"Test F1 (naive splitting): {baseline_f1_naive:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data by user, maintaining click and session distribution\n",
      "Number of users in each set:\n",
      "Train: 76507 (60.0%)\n",
      "Validation: 25502 (20.0%)\n",
      "Test: 25503 (20.0%)\n",
      "\n",
      "Number of sessions in each set:\n",
      "Train: 209531 (60.0%)\n",
      "Validation: 69610 (19.9%)\n",
      "Test: 69883 (20.0%)\n",
      "\n",
      "Click rates in each set:\n",
      "Train: 0.068\n",
      "Validation: 0.068\n",
      "Test: 0.068\n",
      "\n",
      "Average sessions per user in each set:\n",
      "Train: 2.74\n",
      "Validation: 2.73\n",
      "Test: 2.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Splitting data by user, maintaining click and session distribution\")\n",
    "# Create user-level features for stratification\n",
    "user_features = df.groupby('user_id').agg({\n",
    "    'session_id': 'count',  # number of sessions\n",
    "    'is_click': 'sum'       # number of clicks (not rate)\n",
    "}).reset_index()\n",
    "\n",
    "# Create stratification group using actual values\n",
    "user_features['strat_group'] = user_features.apply(\n",
    "    lambda x: f\"sessions_{int(x['session_id'])}_clicks_{int(x['is_click'])}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Identify common and rare groups\n",
    "group_counts = user_features['strat_group'].value_counts()\n",
    "common_groups = group_counts[group_counts >= 6].index\n",
    "\n",
    "# Split users into common and rare groups\n",
    "common_users = user_features[user_features['strat_group'].isin(common_groups)]\n",
    "rare_users = user_features[~user_features['strat_group'].isin(common_groups)]\n",
    "\n",
    "# Split common users with stratification\n",
    "train_users_common, temp_users_common = train_test_split(\n",
    "    common_users['user_id'],\n",
    "    train_size=0.6,\n",
    "    stratify=common_users['strat_group'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_users_common, test_users_common = train_test_split(\n",
    "    temp_users_common,\n",
    "    train_size=0.5,\n",
    "    stratify=common_users.loc[common_users['user_id'].isin(temp_users_common), 'strat_group'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Randomly assign rare users to maintain approximately 60-20-20 split\n",
    "rare_users_shuffled = rare_users['user_id'].sample(frac=1, random_state=42)\n",
    "n_rare = len(rare_users_shuffled)\n",
    "n_train_rare = int(0.6 * n_rare)\n",
    "n_val_rare = int(0.2 * n_rare)\n",
    "\n",
    "train_users_rare = rare_users_shuffled[:n_train_rare]\n",
    "val_users_rare = rare_users_shuffled[n_train_rare:n_train_rare + n_val_rare]\n",
    "test_users_rare = rare_users_shuffled[n_train_rare + n_val_rare:]\n",
    "\n",
    "# Combine common and rare users\n",
    "train_users = pd.concat([train_users_common, train_users_rare])\n",
    "val_users = pd.concat([val_users_common, val_users_rare])\n",
    "test_users = pd.concat([test_users_common, test_users_rare])\n",
    "\n",
    "# Create the final dataframes\n",
    "df_train = df[df['user_id'].isin(train_users)].copy()\n",
    "df_val = df[df['user_id'].isin(val_users)].copy()\n",
    "df_test = df[df['user_id'].isin(test_users)].copy()\n",
    "\n",
    "# Print statistics to verify the split\n",
    "print(\"Number of users in each set:\")\n",
    "print(f\"Train: {len(train_users)} ({len(train_users)/len(user_features):.1%})\")\n",
    "print(f\"Validation: {len(val_users)} ({len(val_users)/len(user_features):.1%})\")\n",
    "print(f\"Test: {len(test_users)} ({len(test_users)/len(user_features):.1%})\")\n",
    "\n",
    "print(\"\\nNumber of sessions in each set:\")\n",
    "print(f\"Train: {len(df_train)} ({len(df_train)/len(df):.1%})\")\n",
    "print(f\"Validation: {len(df_val)} ({len(df_val)/len(df):.1%})\")\n",
    "print(f\"Test: {len(df_test)} ({len(df_test)/len(df):.1%})\")\n",
    "\n",
    "# Verify click distributions are similar\n",
    "print(\"\\nClick rates in each set:\")\n",
    "print(f\"Train: {df_train['is_click'].mean():.3f}\")\n",
    "print(f\"Validation: {df_val['is_click'].mean():.3f}\")\n",
    "print(f\"Test: {df_test['is_click'].mean():.3f}\")\n",
    "\n",
    "# Print distribution of sessions per user in each set\n",
    "print(\"\\nAverage sessions per user in each set:\")\n",
    "print(f\"Train: {df_train.groupby('user_id')['session_id'].count().mean():.2f}\")\n",
    "print(f\"Validation: {df_val.groupby('user_id')['session_id'].count().mean():.2f}\")\n",
    "print(f\"Test: {df_test.groupby('user_id')['session_id'].count().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding product history feature to train set...\n",
      "Adding product history feature to validation set...\n",
      "Adding product history feature to test set...\n"
     ]
    }
   ],
   "source": [
    "# Create feature for whether user has viewed product before\n",
    "def add_product_history(df):\n",
    "    # Sort by user and datetime\n",
    "    df = df.sort_values(['user_id', 'DateTime'])\n",
    "    \n",
    "    # Initialize the new feature\n",
    "    df['product_viewed_before'] = 0\n",
    "    \n",
    "    # For each user\n",
    "    for user_id in df['user_id'].unique():\n",
    "        user_sessions = df[df['user_id'] == user_id]\n",
    "        \n",
    "        # For each session of this user (already sorted chronologically)\n",
    "        for i, (_, current_session) in enumerate(user_sessions.iterrows()):\n",
    "            if i > 0:  # Skip first session\n",
    "                # Get all previous sessions for this user\n",
    "                previous_sessions = user_sessions.iloc[:i]\n",
    "                # Check if current product was viewed in any previous session\n",
    "                if current_session['product'] in previous_sessions['product'].values:\n",
    "                    df.loc[current_session.name, 'product_viewed_before'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add the feature to each dataset\n",
    "print(\"Adding product history feature to train set...\")\n",
    "df_train = add_product_history(df_train)\n",
    "\n",
    "print(\"Adding product history feature to validation set...\")\n",
    "df_val = add_product_history(df_val)\n",
    "\n",
    "print(\"Adding product history feature to test set...\")\n",
    "df_test = add_product_history(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Initialize OneHotEncoder for categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit encoder on train set categorical columns\n",
    "encoder.fit(df_train[cons.CATEGORICAL])\n",
    "\n",
    "# Transform train set\n",
    "train_cat_encoded = encoder.transform(df_train[cons.CATEGORICAL])\n",
    "train_cat_cols = encoder.get_feature_names_out(cons.CATEGORICAL)\n",
    "df_train_encoded = pd.concat([\n",
    "    df_train.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(train_cat_encoded, columns=train_cat_cols, index=df_train.index)\n",
    "], axis=1)\n",
    "\n",
    "# Transform validation set using fitted encoder\n",
    "val_cat_encoded = encoder.transform(df_val[cons.CATEGORICAL]) \n",
    "df_val_encoded = pd.concat([\n",
    "    df_val.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(val_cat_encoded, columns=train_cat_cols, index=df_val.index)\n",
    "], axis=1)\n",
    "\n",
    "# Transform test set using fitted encoder\n",
    "test_cat_encoded = encoder.transform(df_test[cons.CATEGORICAL])\n",
    "df_test_encoded = pd.concat([\n",
    "    df_test.drop(columns=cons.CATEGORICAL),\n",
    "    pd.DataFrame(test_cat_encoded, columns=train_cat_cols, index=df_test.index)\n",
    "], axis=1)\n",
    "\n",
    "df_train = df_train_encoded\n",
    "df_val = df_val_encoded\n",
    "df_test = df_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model F1 Score: 0.078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "\n",
    "# Calculate baseline F1 score without product history feature\n",
    "baseline_features = [col for col in df_train.columns \n",
    "                    if col not in ['user_id', 'session_id', 'DateTime', 'is_click', 'product_viewed_before']]\n",
    "\n",
    "# Initialize baseline model with reasonable parameters\n",
    "baseline_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10, \n",
    "    min_samples_split=5,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "baseline_model.fit(df_train[baseline_features], df_train['is_click'])\n",
    "\n",
    "# Generate predictions\n",
    "baseline_predictions = baseline_model.predict(df_test[baseline_features])\n",
    "\n",
    "# Calculate and print F1 score\n",
    "baseline_avg_precision = average_precision_score(df_test['is_click'], baseline_predictions)\n",
    "print(f\"Baseline Model F1 Score: {baseline_avg_precision:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Average Precision Score: 1.1814\n",
      "Test Set F1 Scores:\n",
      "With product_viewed_before:    0.0800\n",
      "Without product_viewed_before: 0.0784\n",
      "Improvement:                   2.1%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Features to exclude from model\n",
    "exclude_cols = ['user_id', 'session_id', 'DateTime']\n",
    "\n",
    "# Get feature columns with and without product history\n",
    "features_with_history = [col for col in df_train.columns if col not in exclude_cols + ['is_click']]\n",
    "features_without_history = [col for col in features_with_history if col != 'product_viewed_before']\n",
    "\n",
    "# Initialize models\n",
    "model_with_history = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=df_train['is_click'].value_counts()[0]/df_train['is_click'].value_counts()[1],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train and evaluate model with product history\n",
    "model_with_history.fit(df_train[features_with_history], df_train['is_click'])\n",
    "y_pred_with_history = model_with_history.predict(df_test[features_with_history])\n",
    "avg_precision_with_history = average_precision_score(df_test['is_click'], y_pred_with_history)\n",
    "\n",
    "# Calculate and print the normalized average precision score\n",
    "# (Normalizing by the mean click rate helps account for class imbalance)\n",
    "normalized_avg_precision = avg_precision_with_history / df_test['is_click'].mean()\n",
    "print(f\"Normalized Average Precision Score: {normalized_avg_precision:.4f}\")\n",
    "print(\"Test Set F1 Scores:\")\n",
    "print(f\"With product_viewed_before:    {avg_precision_with_history:.4f}\")\n",
    "print(f\"Without product_viewed_before: {baseline_avg_precision:.4f}\")\n",
    "print(f\"Improvement:                   {((avg_precision_with_history - baseline_avg_precision) / baseline_avg_precision * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Click-through Rate Analysis:\n",
      "Overall CTR: 0.0677\n",
      "CTR when product viewed before: 0.0492\n",
      "Relative lift when previously viewed: 27.4%\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall CTR (mean click-through rate)\n",
    "overall_ctr = df_train['is_click'].mean()\n",
    "\n",
    "# Calculate CTR conditional on product being viewed before\n",
    "ctr_viewed_before = df_train[df_train['product_viewed_before'] == 1]['is_click'].mean()\n",
    "ctr_not_viewed_before = df_train[df_train['product_viewed_before'] == 0]['is_click'].mean()\n",
    "\n",
    "print(\"\\nClick-through Rate Analysis:\")\n",
    "print(f\"Overall CTR: {overall_ctr:.4f}\")\n",
    "print(f\"CTR when product viewed before: {ctr_viewed_before:.4f}\")\n",
    "print(f\"Relative lift when previously viewed: {((overall_ctr - ctr_viewed_before) / overall_ctr * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Click-through Rate Analysis for Users with Multiple Sessions (2nd session onwards):\n",
      "CTR when had session within last hour: 0.0489\n",
      "CTR when no recent session: 0.0591\n",
      "Relative lift with recent session: -17.4%\n",
      "\n",
      "Chi-square statistic: 60.46\n",
      "p-value: 7.5225e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/zfsc1dvx7wj0x53jmvndd7z80000gn/T/ipykernel_2351/2820751937.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['had_recent_session'] = (filtered_df['hours_since_prev'] <= 1).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Sort by user and datetime\n",
    "df_train = df_train.sort_values(['user_id', 'DateTime'])\n",
    "\n",
    "# Calculate time since previous session for each user\n",
    "df_train['time_since_prev'] = df_train.groupby('user_id')['DateTime'].diff()\n",
    "\n",
    "# Convert timedelta to hours\n",
    "df_train['hours_since_prev'] = df_train['time_since_prev'].dt.total_seconds() / 360\n",
    "\n",
    "# Get users with more than 2 sessions\n",
    "users_multi_sessions = df_train.groupby('user_id').size()\n",
    "users_multi_sessions = users_multi_sessions[users_multi_sessions > 2].index\n",
    "\n",
    "# Filter for those users and sessions after the first one\n",
    "filtered_df = df_train[\n",
    "    (df_train['user_id'].isin(users_multi_sessions)) & \n",
    "    (df_train['hours_since_prev'].notna())\n",
    "]\n",
    "\n",
    "# Create binary features\n",
    "filtered_df['had_recent_session'] = (filtered_df['hours_since_prev'] <= 1).astype(int)\n",
    "\n",
    "# Calculate CTR for sessions with and without recent activity\n",
    "ctr_with_recent = filtered_df[filtered_df['had_recent_session'] == 1]['is_click'].mean()\n",
    "ctr_without_recent = filtered_df[filtered_df['had_recent_session'] == 0]['is_click'].mean()\n",
    "\n",
    "print(\"\\nClick-through Rate Analysis for Users with Multiple Sessions (2nd session onwards):\")\n",
    "print(f\"CTR when had session within last hour: {ctr_with_recent:.4f}\")\n",
    "print(f\"CTR when no recent session: {ctr_without_recent:.4f}\")\n",
    "print(f\"Relative lift with recent session: {((ctr_with_recent - ctr_without_recent) / ctr_without_recent * 100):.1f}%\")\n",
    "\n",
    "# Calculate statistical significance using chi-square test\n",
    "contingency = pd.crosstab(filtered_df['had_recent_session'], filtered_df['is_click'])\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2, p_value = chi2_contingency(contingency)[:2]\n",
    "\n",
    "print(f\"\\nChi-square statistic: {chi2:.2f}\")\n",
    "print(f\"p-value: {p_value:.4e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
